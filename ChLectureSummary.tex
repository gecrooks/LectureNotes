% !TEX encoding = UTF-8 Unicode 
% !TEX TS-program = xelatex

\documentclass[Lectures.tex]{subfiles}




\begin{document}


\section{Lecture Summaries}

\subsection{Probability and ensembles}
\begin{itemize}
\item Probabilities are contextual (e.g. probability of a fair coin flip)
\item Probabilities depend on compute power (e.g. probability that a number is prime) 
\item Human's are often bad at probabilities (Monte Hall problem, Three card problem)
\item Probability notation
\item Ensembles
\item random variables
\item joint, marginal, and conditional probabilities
\item Bayes rule
\end{itemize}

\subsection{Bits and Bytes}
\begin{itemize}
\item Entropy, $S(A) = -\sum_a P_A(a) \log P_A(a)$
\item review history (Clausis, Boltzmann, Gibbs, Shannon, Jaynes, Bennet and Landauer)
\item review how logs work
\item units (bits, nats, ban, deciban)
%\item prove ln omega [jensens inequality.)
\item units, kT
\item orders of magnitude
\end{itemize}

\subsubsection*{Further Reading}
Review Chapter 2 of \citem{Cover2006a}.

\noindent
Optional: Shannon's original paper is well worth reading,~\citem{Shannon1948a}.

   % 8. axiomization derivative
  %  9. Reading: shannons original paper
 
%5. Relative Entropy
%    1. Summary of previous lecture
%    2. relative entropy
%        1. naming
%        2. notation
%        3. intuition
%        4. not a distance
%        5. related to mutual information
%        6. cross entropy
%    3. Jensen’s inequality
%        1. Statement
%        2. Convex and concave functions
%        3. <exp x> exp(<x>), and log
%        4. Relative entropy is non-negative
%        5. Max S(A) = ln N
%    4. Relative entropy and free energy
%    5. differential entropy
%        1. normal
%        2. change of variable problems
%        3. problems with classical entropy of continuous system
%        4. differential relative entropy invariant

\subsection{Entropy and Information}

\begin{itemize}
\item joint, marginal, conditional entropies
\item mutual information 
%        1. definition
%        2. also called shannon information
\item nature of information. Coin example, correlations 
\item information diagrams, 
%        1. mapping to sets intersection, ect
%    5. 3 variable xor example
\item 3 variable mutual information
\item information diagram, 3 variable
\end{itemize}

%\subsubsection*{Further Reading}
% Chapter 2 of \citem{Cover2006a}.
%


% \subsection{Relative Entropy}
\begin{itemize}

\item relative entropy
\begin{itemize}
	\item  naming
	\item notation
	\item  intuition
	\item  not a distance
	\item  related to mutual information
	\item  cross entropy
\end{itemize}
\item Jensen’s inequality
\begin{itemize}
	\item	 Statement
	\item Convex and concave functions
	\item $<\exp x> \ge exp(<x>)$
	\item Relative entropy is non-negative
	\item Mutual information is non-negative
	\item Max $S(A) = ln N$
\end{itemize}

\item Relative entropy and free energy
\item differential entropy
\begin{itemize}
	\item normal
	\item change of variable problems
	\item problems with classical entropy of continuous system
	\item differential relative entropy invariant
\end{itemize}
\end{itemize}
\subsubsection*{Further Reading}
Chapter 2 of \citem{Cover2006a}. See also \citem{_info}, which summaries the information measures we covered, as well as many other less common measures of information.


\subsection{Maxwell's demon and Szilard's Engine}

\subsection{Thermodynamics and Stat. Mech.}

\subsection{Free Energy, work and heat}

\subsection{Discrete Time Markov Chains (DTMC)}


\subsubsection*{Further Reading}
The book by Norris provides a good introduction to discrete and continuous time Markov Chains \citem{Norris1997a}.


\subsection{Jarzynski Identity}


\subsection{Fluctuation Theorems }


\subsection{Continuous Time Markov Chains (CTMC)}


\subsubsection*{Further Reading}
The book by Norris provides a good introduction to discrete and continuous time Markov Chains \citem{Norris1997a}.



\end{document}